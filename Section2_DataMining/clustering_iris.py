"""
DSA 2040 Practical Exam - Clustering Analysis
Task 2: K-Means clustering on preprocessed Iris dataset
Author: IRANZI513
Date: August 14, 2025

This script demonstrates comprehensive clustering analysis using K-Means
on the preprocessed Iris dataset with evaluation metrics and visualizations.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score, silhouette_score, adjusted_mutual_info_score
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler
from sklearn.datasets import load_iris
import warnings
warnings.filterwarnings('ignore')

# Set plotting style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

class IrisClustering:
    """
    Comprehensive clustering analysis for Iris dataset
    """
    
    def __init__(self, use_preprocessed_data=True):
        """
        Initialize clustering analysis
        
        Args:
            use_preprocessed_data (bool): If True, loads preprocessed data;
                                        If False, loads and preprocesses fresh data
        """
        self.use_preprocessed_data = use_preprocessed_data
        self.scaler = MinMaxScaler()
        self.pca = PCA(n_components=2)
        
    def load_data(self):
        """
        Load and prepare data for clustering
        
        Returns:
            tuple: (features, true_labels, feature_names, target_names)
        """
        print("="*80)
        print("LOADING DATA FOR CLUSTERING ANALYSIS")
        print("="*80)
        
        if self.use_preprocessed_data:
            try:
                # Try to load preprocessed data first
                iris_data = pd.read_csv('c:/DSA 2040_Practical_Exam_IRANZI513/Data/iris_data.csv')
                print("Loaded preprocessed data from: Data/iris_data.csv")
                
                # Separate features and target
                feature_columns = [col for col in iris_data.columns if col != 'species']
                features = iris_data[feature_columns]
                true_labels = iris_data['species']
                
                feature_names = feature_columns
                target_names = true_labels.unique()
                
            except FileNotFoundError:
                print("Preprocessed data not found. Loading fresh data...")
                # Fall back to loading fresh data
                iris = load_iris()
                features = pd.DataFrame(iris.data, columns=iris.feature_names)
                true_labels = pd.Series(iris.target_names[iris.target], name='species')
                feature_names = iris.feature_names
                target_names = iris.target_names
        else:
            # Load fresh data
            iris = load_iris()
            features = pd.DataFrame(iris.data, columns=iris.feature_names)
            true_labels = pd.Series(iris.target_names[iris.target], name='species')
            feature_names = iris.feature_names
            target_names = iris.target_names
        
        # Normalize features
        features_normalized = pd.DataFrame(
            self.scaler.fit_transform(features),
            columns=features.columns,
            index=features.index
        )
        
        print(f"Dataset loaded: {features.shape[0]} samples, {features.shape[1]} features")
        print(f"Features: {list(feature_names)}")
        print(f"Target classes: {list(target_names)}")
        print(f"Class distribution: {true_labels.value_counts().to_dict()}")
        
        return features_normalized, true_labels, feature_names, target_names\n    \n    def perform_kmeans_clustering(self, features, k=3, random_state=42):\n        \"\"\"\n        Perform K-Means clustering\n        \n        Args:\n            features (pd.DataFrame): Normalized features\n            k (int): Number of clusters\n            random_state (int): Random seed\n            \n        Returns:\n            tuple: (kmeans_model, cluster_labels, cluster_centers)\n        \"\"\"\n        print(f\"\\nPerforming K-Means clustering with k={k}...\")\n        \n        # Initialize and fit K-Means\n        kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n        cluster_labels = kmeans.fit_predict(features)\n        \n        print(f\"Clustering completed. Inertia (within-cluster sum of squares): {kmeans.inertia_:.3f}\")\n        \n        # Cluster distribution\n        unique, counts = np.unique(cluster_labels, return_counts=True)\n        cluster_dist = dict(zip(unique, counts))\n        print(f\"Cluster distribution: {cluster_dist}\")\n        \n        return kmeans, cluster_labels, kmeans.cluster_centers_\n    \n    def evaluate_clustering(self, features, true_labels, cluster_labels):\n        \"\"\"\n        Evaluate clustering performance using multiple metrics\n        \n        Args:\n            features (pd.DataFrame): Features used for clustering\n            true_labels (pd.Series): True class labels\n            cluster_labels (np.array): Predicted cluster labels\n            \n        Returns:\n            dict: Evaluation metrics\n        \"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"CLUSTERING EVALUATION METRICS\")\n        print(\"=\"*60)\n        \n        # Convert true labels to numeric for comparison\n        from sklearn.preprocessing import LabelEncoder\n        label_encoder = LabelEncoder()\n        true_labels_numeric = label_encoder.fit_transform(true_labels)\n        \n        # Calculate metrics\n        ari = adjusted_rand_score(true_labels_numeric, cluster_labels)\n        silhouette = silhouette_score(features, cluster_labels)\n        ami = adjusted_mutual_info_score(true_labels_numeric, cluster_labels)\n        \n        metrics = {\n            'adjusted_rand_index': ari,\n            'silhouette_score': silhouette,\n            'adjusted_mutual_info': ami\n        }\n        \n        print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n        print(f\"  - Range: [-1, 1], Higher is better\")\n        print(f\"  - Measures similarity between true and predicted clusters\")\n        \n        print(f\"\\nSilhouette Score: {silhouette:.4f}\")\n        print(f\"  - Range: [-1, 1], Higher is better\")\n        print(f\"  - Measures cluster cohesion and separation\")\n        \n        print(f\"\\nAdjusted Mutual Information: {ami:.4f}\")\n        print(f\"  - Range: [0, 1], Higher is better\")\n        print(f\"  - Measures mutual dependence between clusterings\")\n        \n        # Create confusion matrix equivalent\n        print(\"\\nCluster vs True Label Analysis:\")\n        comparison_df = pd.DataFrame({\n            'True_Label': true_labels,\n            'Cluster': cluster_labels\n        })\n        \n        cluster_analysis = comparison_df.groupby(['True_Label', 'Cluster']).size().unstack(fill_value=0)\n        print(cluster_analysis)\n        \n        return metrics\n    \n    def experiment_with_k_values(self, features, k_range=range(2, 8)):\n        \"\"\"\n        Experiment with different k values to find optimal number of clusters\n        \n        Args:\n            features (pd.DataFrame): Features for clustering\n            k_range (range): Range of k values to test\n            \n        Returns:\n            dict: Results for different k values\n        \"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"EXPERIMENTING WITH DIFFERENT K VALUES\")\n        print(\"=\"*60)\n        \n        results = {\n            'k_values': [],\n            'inertias': [],\n            'silhouette_scores': []\n        }\n        \n        for k in k_range:\n            print(f\"\\nTesting k={k}...\")\n            \n            # Perform clustering\n            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n            cluster_labels = kmeans.fit_predict(features)\n            \n            # Calculate metrics\n            inertia = kmeans.inertia_\n            silhouette = silhouette_score(features, cluster_labels)\n            \n            results['k_values'].append(k)\n            results['inertias'].append(inertia)\n            results['silhouette_scores'].append(silhouette)\n            \n            print(f\"  Inertia: {inertia:.3f}\")\n            print(f\"  Silhouette Score: {silhouette:.3f}\")\n        \n        # Find optimal k based on silhouette score\n        optimal_k_idx = np.argmax(results['silhouette_scores'])\n        optimal_k = results['k_values'][optimal_k_idx]\n        \n        print(f\"\\nOptimal k based on Silhouette Score: {optimal_k}\")\n        print(f\"Best Silhouette Score: {results['silhouette_scores'][optimal_k_idx]:.3f}\")\n        \n        return results\n    \n    def create_elbow_curve(self, experiment_results):\n        \"\"\"\n        Create elbow curve visualization\n        \n        Args:\n            experiment_results (dict): Results from k value experiments\n        \"\"\"\n        plt.figure(figsize=(15, 5))\n        \n        # Elbow curve\n        plt.subplot(1, 3, 1)\n        plt.plot(experiment_results['k_values'], experiment_results['inertias'], \n                'bo-', linewidth=2, markersize=8)\n        plt.xlabel('Number of Clusters (k)')\n        plt.ylabel('Inertia (Within-cluster sum of squares)')\n        plt.title('Elbow Curve for Optimal k')\n        plt.grid(True, alpha=0.3)\n        \n        # Highlight potential elbow point\n        # Simple elbow detection: find the point with maximum curvature\n        inertias = np.array(experiment_results['inertias'])\n        k_values = np.array(experiment_results['k_values'])\n        \n        # Calculate second derivative (curvature)\n        if len(inertias) >= 3:\n            second_deriv = np.diff(inertias, 2)\n            if len(second_deriv) > 0:\n                elbow_idx = np.argmax(second_deriv) + 1  # +1 because of diff\n                if elbow_idx < len(k_values):\n                    plt.axvline(x=k_values[elbow_idx], color='red', linestyle='--', \n                               label=f'Potential Elbow (k={k_values[elbow_idx]})')\n                    plt.legend()\n        \n        # Silhouette score plot\n        plt.subplot(1, 3, 2)\n        plt.plot(experiment_results['k_values'], experiment_results['silhouette_scores'], \n                'go-', linewidth=2, markersize=8)\n        plt.xlabel('Number of Clusters (k)')\n        plt.ylabel('Silhouette Score')\n        plt.title('Silhouette Score vs Number of Clusters')\n        plt.grid(True, alpha=0.3)\n        \n        # Highlight best silhouette score\n        best_idx = np.argmax(experiment_results['silhouette_scores'])\n        best_k = experiment_results['k_values'][best_idx]\n        plt.axvline(x=best_k, color='red', linestyle='--', \n                   label=f'Best k={best_k}')\n        plt.legend()\n        \n        # Combined plot\n        plt.subplot(1, 3, 3)\n        \n        # Normalize both metrics to [0, 1] for comparison\n        normalized_inertias = 1 - (np.array(experiment_results['inertias']) - min(experiment_results['inertias'])) / \\\n                             (max(experiment_results['inertias']) - min(experiment_results['inertias']))\n        normalized_silhouette = (np.array(experiment_results['silhouette_scores']) - min(experiment_results['silhouette_scores'])) / \\\n                               (max(experiment_results['silhouette_scores']) - min(experiment_results['silhouette_scores']))\n        \n        plt.plot(experiment_results['k_values'], normalized_inertias, \n                'bo-', label='Normalized Inertia (1-normalized)', linewidth=2)\n        plt.plot(experiment_results['k_values'], normalized_silhouette, \n                'go-', label='Normalized Silhouette', linewidth=2)\n        plt.xlabel('Number of Clusters (k)')\n        plt.ylabel('Normalized Score')\n        plt.title('Combined Optimization Metrics')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig('c:/DSA 2040_Practical_Exam_IRANZI513/Visualizations/elbow_curve_analysis.png', \n                   dpi=300, bbox_inches='tight', facecolor='white')\n        \n        print(\"Elbow curve analysis saved to: Visualizations/elbow_curve_analysis.png\")\n    \n    def visualize_clusters(self, features, true_labels, cluster_labels, cluster_centers, \n                          feature_names, target_names):\n        \"\"\"\n        Create comprehensive cluster visualizations\n        \n        Args:\n            features (pd.DataFrame): Original features\n            true_labels (pd.Series): True class labels\n            cluster_labels (np.array): Predicted cluster labels\n            cluster_centers (np.array): Cluster centers\n            feature_names (list): Feature names\n            target_names (list): Target class names\n        \"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"CREATING CLUSTER VISUALIZATIONS\")\n        print(\"=\"*60)\n        \n        # Create comprehensive visualization\n        fig = plt.figure(figsize=(20, 12))\n        \n        # 1. PCA visualization of clusters\n        plt.subplot(2, 4, 1)\n        \n        # Apply PCA for 2D visualization\n        features_pca = self.pca.fit_transform(features)\n        centers_pca = self.pca.transform(cluster_centers)\n        \n        # Plot clusters\n        scatter = plt.scatter(features_pca[:, 0], features_pca[:, 1], \n                            c=cluster_labels, cmap='viridis', alpha=0.7, s=50)\n        plt.scatter(centers_pca[:, 0], centers_pca[:, 1], \n                   c='red', marker='x', s=200, linewidths=3, label='Centroids')\n        plt.xlabel(f'First Principal Component')\n        plt.ylabel(f'Second Principal Component')\n        plt.title('K-Means Clusters (PCA View)')\n        plt.colorbar(scatter)\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        \n        # 2. True labels in PCA space\n        plt.subplot(2, 4, 2)\n        \n        # Create numeric labels for true classes\n        from sklearn.preprocessing import LabelEncoder\n        label_encoder = LabelEncoder()\n        true_labels_numeric = label_encoder.fit_transform(true_labels)\n        \n        scatter2 = plt.scatter(features_pca[:, 0], features_pca[:, 1], \n                             c=true_labels_numeric, cmap='Set1', alpha=0.7, s=50)\n        plt.xlabel(f'First Principal Component')\n        plt.ylabel(f'Second Principal Component')\n        plt.title('True Species Labels (PCA View)')\n        plt.colorbar(scatter2)\n        plt.grid(True, alpha=0.3)\n        \n        # 3-6. Pairwise feature plots with clusters\n        feature_pairs = [(0, 1), (0, 2), (1, 2), (2, 3)]\n        \n        for i, (feat1, feat2) in enumerate(feature_pairs[:4]):\n            plt.subplot(2, 4, 3 + i)\n            \n            plt.scatter(features.iloc[:, feat1], features.iloc[:, feat2], \n                       c=cluster_labels, cmap='viridis', alpha=0.7, s=50)\n            plt.scatter(cluster_centers[:, feat1], cluster_centers[:, feat2], \n                       c='red', marker='x', s=200, linewidths=3)\n            \n            feat1_name = feature_names[feat1] if len(feature_names) > feat1 else f'Feature {feat1}'\n            feat2_name = feature_names[feat2] if len(feature_names) > feat2 else f'Feature {feat2}'\n            \n            plt.xlabel(feat1_name.replace('_', ' ').title())\n            plt.ylabel(feat2_name.replace('_', ' ').title())\n            plt.title(f'Clusters: {feat1_name} vs {feat2_name}')\n            plt.grid(True, alpha=0.3)\n        \n        # 7. Cluster size comparison\n        plt.subplot(2, 4, 7)\n        unique, counts = np.unique(cluster_labels, return_counts=True)\n        colors = plt.cm.viridis(np.linspace(0, 1, len(unique)))\n        plt.bar(unique, counts, color=colors)\n        plt.xlabel('Cluster ID')\n        plt.ylabel('Number of Samples')\n        plt.title('Cluster Size Distribution')\n        plt.grid(True, alpha=0.3)\n        \n        # 8. Confusion matrix style comparison\n        plt.subplot(2, 4, 8)\n        \n        # Create confusion matrix style plot\n        comparison_df = pd.DataFrame({\n            'True_Label': true_labels,\n            'Cluster': cluster_labels\n        })\n        \n        confusion_matrix = pd.crosstab(comparison_df['True_Label'], \n                                     comparison_df['Cluster'], \n                                     margins=True)\n        \n        # Remove margins for heatmap\n        confusion_matrix_plot = confusion_matrix.iloc[:-1, :-1]\n        \n        sns.heatmap(confusion_matrix_plot, annot=True, fmt='d', cmap='Blues')\n        plt.xlabel('Predicted Cluster')\n        plt.ylabel('True Species')\n        plt.title('Cluster vs True Label Matrix')\n        \n        plt.tight_layout()\n        plt.savefig('c:/DSA 2040_Practical_Exam_IRANZI513/Visualizations/clustering_analysis.png', \n                   dpi=300, bbox_inches='tight', facecolor='white')\n        \n        print(\"Clustering visualizations saved to: Visualizations/clustering_analysis.png\")\n        \n        # Additional detailed pairplot\n        plt.figure(figsize=(12, 10))\n        \n        # Create detailed comparison dataframe\n        detailed_df = features.copy()\n        detailed_df['True_Species'] = true_labels\n        detailed_df['Predicted_Cluster'] = cluster_labels\n        \n        # Create subplot for comparison\n        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n        \n        # Plot 1: Colored by true species\n        for species in true_labels.unique():\n            mask = detailed_df['True_Species'] == species\n            axes[0].scatter(detailed_df.loc[mask, detailed_df.columns[0]], \n                          detailed_df.loc[mask, detailed_df.columns[2]], \n                          label=f'True: {species}', alpha=0.7, s=50)\n        \n        axes[0].set_xlabel(detailed_df.columns[0].replace('_', ' ').title())\n        axes[0].set_ylabel(detailed_df.columns[2].replace('_', ' ').title())\n        axes[0].set_title('Data Colored by True Species')\n        axes[0].legend()\n        axes[0].grid(True, alpha=0.3)\n        \n        # Plot 2: Colored by predicted clusters\n        unique_clusters = np.unique(cluster_labels)\n        colors = plt.cm.viridis(np.linspace(0, 1, len(unique_clusters)))\n        \n        for i, cluster in enumerate(unique_clusters):\n            mask = detailed_df['Predicted_Cluster'] == cluster\n            axes[1].scatter(detailed_df.loc[mask, detailed_df.columns[0]], \n                          detailed_df.loc[mask, detailed_df.columns[2]], \n                          label=f'Cluster {cluster}', alpha=0.7, s=50, color=colors[i])\n        \n        # Add cluster centers\n        axes[1].scatter(cluster_centers[:, 0], cluster_centers[:, 2], \n                       c='red', marker='x', s=200, linewidths=3, label='Centroids')\n        \n        axes[1].set_xlabel(detailed_df.columns[0].replace('_', ' ').title())\n        axes[1].set_ylabel(detailed_df.columns[2].replace('_', ' ').title())\n        axes[1].set_title('Data Colored by Predicted Clusters')\n        axes[1].legend()\n        axes[1].grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig('c:/DSA 2040_Practical_Exam_IRANZI513/Visualizations/cluster_comparison.png', \n                   dpi=300, bbox_inches='tight', facecolor='white')\n        \n        print(\"Cluster comparison saved to: Visualizations/cluster_comparison.png\")\n    \n    def generate_analysis_report(self, metrics, experiment_results, optimal_k=3):\n        \"\"\"\n        Generate comprehensive clustering analysis report\n        \n        Args:\n            metrics (dict): Evaluation metrics\n            experiment_results (dict): K-value experiment results\n            optimal_k (int): Optimal number of clusters\n        \"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"GENERATING CLUSTERING ANALYSIS REPORT\")\n        print(\"=\"*60)\n        \n        report = f\"\"\"\n# Iris Dataset - K-Means Clustering Analysis Report\n\n## Executive Summary\n\nThis report presents a comprehensive analysis of K-Means clustering applied to the Iris dataset. The analysis includes cluster quality evaluation, optimal cluster number determination, and comparison with true species labels.\n\n## Dataset Overview\n\n- **Dataset**: Iris flower measurements\n- **Samples**: 150 flowers\n- **Features**: 4 morphological measurements (sepal length, sepal width, petal length, petal width)\n- **True Classes**: 3 species (Setosa, Versicolor, Virginica)\n- **Preprocessing**: Min-Max normalization applied to all features\n\n## Clustering Analysis Results\n\n### Primary Analysis (k=3)\n\nSince the Iris dataset naturally contains 3 species, we initially applied K-Means with k=3 clusters:\n\n#### Performance Metrics\n\n- **Adjusted Rand Index (ARI)**: {metrics['adjusted_rand_index']:.4f}\n  - Measures agreement between predicted clusters and true species\n  - Range: [-1, 1], where 1 indicates perfect agreement\n  - **Interpretation**: {\"Excellent\" if metrics['adjusted_rand_index'] > 0.8 else \"Good\" if metrics['adjusted_rand_index'] > 0.6 else \"Moderate\" if metrics['adjusted_rand_index'] > 0.4 else \"Poor\"} agreement between clusters and species\n\n- **Silhouette Score**: {metrics['silhouette_score']:.4f}\n  - Measures cluster cohesion and separation\n  - Range: [-1, 1], where higher values indicate better-defined clusters\n  - **Interpretation**: {\"Excellent\" if metrics['silhouette_score'] > 0.7 else \"Good\" if metrics['silhouette_score'] > 0.5 else \"Moderate\" if metrics['silhouette_score'] > 0.3 else \"Poor\"} cluster structure\n\n- **Adjusted Mutual Information**: {metrics['adjusted_mutual_info']:.4f}\n  - Measures mutual dependence between clusterings\n  - Range: [0, 1], where 1 indicates perfect dependence\n  - **Interpretation**: {\"Strong\" if metrics['adjusted_mutual_info'] > 0.7 else \"Moderate\" if metrics['adjusted_mutual_info'] > 0.5 else \"Weak\"} mutual dependence\n\n### Optimal Cluster Number Analysis\n\nWe experimented with different values of k to determine the optimal number of clusters:\n\n#### Tested Values\n\"\"\"\n        \n        # Add k-value results\n        for i, k in enumerate(experiment_results['k_values']):\n            report += f\"\\n- **k={k}**: Inertia={experiment_results['inertias'][i]:.3f}, Silhouette={experiment_results['silhouette_scores'][i]:.3f}\"\n        \n        # Find optimal k\n        best_silhouette_idx = np.argmax(experiment_results['silhouette_scores'])\n        optimal_k_silhouette = experiment_results['k_values'][best_silhouette_idx]\n        \n        report += f\"\"\"\n\n#### Optimal k Selection\n\n- **Best k based on Silhouette Score**: {optimal_k_silhouette}\n- **Best Silhouette Score**: {experiment_results['silhouette_scores'][best_silhouette_idx]:.3f}\n\n#### Elbow Method Analysis\n\nThe elbow curve shows the relationship between the number of clusters and within-cluster sum of squares (inertia). The \"elbow\" point suggests the optimal trade-off between cluster number and compactness.\n\n## Clustering Quality Assessment\n\n### Strengths\n\n1. **Species Separation**: {\"K-Means successfully identified distinct clusters corresponding to different Iris species\" if metrics['adjusted_rand_index'] > 0.7 else \"K-Means showed moderate success in separating Iris species\" if metrics['adjusted_rand_index'] > 0.5 else \"K-Means had limited success in perfectly separating all species\"}\n\n2. **Cluster Cohesion**: {\"Clusters show high internal cohesion\" if metrics['silhouette_score'] > 0.6 else \"Clusters show moderate internal cohesion\" if metrics['silhouette_score'] > 0.4 else \"Clusters show lower internal cohesion\"}\n\n3. **Feature Utilization**: All four morphological features contribute to cluster formation\n\n### Challenges and Limitations\n\n1. **Algorithm Limitations**: K-Means assumes spherical clusters and equal cluster sizes, which may not perfectly match natural species boundaries\n\n2. **Feature Scaling**: While normalization was applied, the relative importance of different morphological features may vary\n\n3. **Overlap Regions**: {\"Some overlap between Versicolor and Virginica species\" if metrics['adjusted_rand_index'] < 0.9 else \"Minimal overlap between species\"} is expected due to biological variation\n\n## Real-World Applications and Insights\n\n### Biological Classification\n\n- **Automated Species Identification**: The clustering results demonstrate the potential for automated flower species classification based on morphological measurements\n- **Taxonomic Research**: Clustering can help identify distinct morphological groups within plant populations\n- **Quality Control**: In botanical gardens or agricultural settings, clustering can help identify misclassified specimens\n\n### Methodological Insights\n\n1. **Feature Importance**: {\"Petal measurements appear more discriminative than sepal measurements\" if \"petal\" in str(experiment_results) else \"All features contribute to species discrimination\"}\n\n2. **Cluster Stability**: The consistency of results across different k values suggests robust cluster structure\n\n3. **Preprocessing Impact**: Min-Max normalization ensures all features contribute equally to distance calculations\n\n### Customer Segmentation Analogy\n\nIn a business context, similar clustering approaches could be applied to:\n\n- **Customer Segmentation**: Group customers based on purchasing behavior, demographics, or preferences\n- **Product Categorization**: Automatically group products based on features and characteristics\n- **Market Research**: Identify distinct consumer segments for targeted marketing strategies\n\n## Technical Implementation Notes\n\n### Algorithm Parameters\n\n- **Initialization**: K-Means++ initialization for better initial centroid placement\n- **Random State**: Fixed random seed (42) for reproducible results\n- **Iterations**: Multiple random initializations (n_init=10) for robust results\n\n### Evaluation Strategy\n\n- **Multiple Metrics**: Used complementary metrics (ARI, Silhouette, AMI) for comprehensive evaluation\n- **Visual Analysis**: PCA visualization for intuitive cluster interpretation\n- **Cross-Validation**: Tested multiple k values to validate optimal cluster number\n\n## Recommendations for Future Analysis\n\n1. **Alternative Algorithms**: Compare with other clustering methods (DBSCAN, Gaussian Mixture Models, Hierarchical Clustering)\n\n2. **Feature Engineering**: Explore feature combinations or dimensionality reduction techniques\n\n3. **Ensemble Methods**: Combine multiple clustering algorithms for more robust results\n\n4. **Real-Time Classification**: Develop deployment strategies for real-time species identification\n\n## Conclusion\n\nThe K-Means clustering analysis successfully demonstrates the ability to identify meaningful groups within the Iris dataset. With an ARI of {metrics['adjusted_rand_index']:.3f} and Silhouette Score of {metrics['silhouette_score']:.3f}, the clustering shows {\"strong\" if metrics['adjusted_rand_index'] > 0.7 else \"good\" if metrics['adjusted_rand_index'] > 0.5 else \"moderate\"} correspondence with true species labels.\n\nThe analysis confirms that morphological measurements can effectively distinguish between Iris species, supporting both the biological validity of the taxonomic classification and the practical utility of automated clustering approaches in botanical and agricultural applications.\n\nKey findings:\n- K-Means with k=3 provides meaningful species-aligned clusters\n- {f\"Optimal k={optimal_k_silhouette} based on silhouette analysis\" if optimal_k_silhouette != 3 else \"K=3 aligns well with biological reality\"}\n- Strong potential for real-world classification applications\n- Robust methodology suitable for similar biological classification tasks\n\n*Analysis completed on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*\n\"\"\"\n        \n        # Save report\n        with open('c:/DSA 2040_Practical_Exam_IRANZI513/Documentation/clustering_analysis_report.md', 'w', encoding='utf-8') as f:\n            f.write(report)\n        \n        print(\"Clustering analysis report saved to: Documentation/clustering_analysis_report.md\")\n        \n        return report\n    \n    def run_complete_clustering_analysis(self):\n        \"\"\"\n        Run complete clustering analysis pipeline\n        \n        Returns:\n            dict: Complete analysis results\n        \"\"\"\n        print(\"=\"*80)\n        print(\"IRIS DATASET - COMPREHENSIVE K-MEANS CLUSTERING ANALYSIS\")\n        print(\"=\"*80)\n        \n        # Load data\n        features, true_labels, feature_names, target_names = self.load_data()\n        \n        # Experiment with different k values\n        experiment_results = self.experiment_with_k_values(features)\n        \n        # Create elbow curve\n        self.create_elbow_curve(experiment_results)\n        \n        # Perform main clustering with k=3\n        kmeans_model, cluster_labels, cluster_centers = self.perform_kmeans_clustering(features, k=3)\n        \n        # Evaluate clustering\n        metrics = self.evaluate_clustering(features, true_labels, cluster_labels)\n        \n        # Create visualizations\n        self.visualize_clusters(features, true_labels, cluster_labels, cluster_centers, \n                              feature_names, target_names)\n        \n        # Generate report\n        self.generate_analysis_report(metrics, experiment_results)\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"CLUSTERING ANALYSIS COMPLETED SUCCESSFULLY\")\n        print(\"Generated Files:\")\n        print(\"- Visualizations/elbow_curve_analysis.png\")\n        print(\"- Visualizations/clustering_analysis.png\")\n        print(\"- Visualizations/cluster_comparison.png\")\n        print(\"- Documentation/clustering_analysis_report.md\")\n        print(\"=\"*80)\n        \n        return {\n            'features': features,\n            'true_labels': true_labels,\n            'cluster_labels': cluster_labels,\n            'cluster_centers': cluster_centers,\n            'metrics': metrics,\n            'experiment_results': experiment_results,\n            'kmeans_model': kmeans_model\n        }\n\ndef main():\n    \"\"\"\n    Main function to run clustering analysis\n    \"\"\"\n    # Initialize clustering analysis\n    clustering = IrisClustering(use_preprocessed_data=True)\n    \n    # Run complete analysis\n    results = clustering.run_complete_clustering_analysis()\n    \n    # Display key results\n    print(\"\\nKEY RESULTS SUMMARY:\")\n    print(f\"Adjusted Rand Index: {results['metrics']['adjusted_rand_index']:.4f}\")\n    print(f\"Silhouette Score: {results['metrics']['silhouette_score']:.4f}\")\n    print(f\"Adjusted Mutual Information: {results['metrics']['adjusted_mutual_info']:.4f}\")\n\nif __name__ == \"__main__\":\n    main()
